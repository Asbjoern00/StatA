---
title: "BS8 and BS10"
output: html_document
date: "2024-01-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(ggplot2)
library(dplyr)
library(brms)
load("mandatory3.Rdata")
after <- subset(vanData, law==1)
before <- subset(vanData, law==0)
```

# A

## 1.

We fit a Poisson model to the data under the assumption that $Y_i \sim poiss(\lambda)$ are i.i.d. Recall that the poisson family uses the log-link by default. We can specify the identity link to get on normal scale
```{r}
fitted <- glm(y ~ 1, family = poisson("identity"), data = before)
fitted
```
and the following confidence interval
```{r}
fitted %>% coef()
```

## 2.

Up to proportionality the likelihood for $(y_1, y_2, \cdots, y_n)$ given $\lambda$ is
$$
p(y|\lambda) = \prod_{i = 1}^n \frac{\lambda^{y_i}}{y_i!}\exp(\lambda) \propto \lambda^{\sum_{i = 1}^n y_i} \exp(-n\lambda) = \lambda^{n \bar{y}} \exp(-n\lambda)
$$
With $\bar{y}$ the average of the $y$'s. This shows that $\bar{y}$ is sufficient. If we have a uniform prior on $\lambda$ the posterior is
$$
p(\lambda| \bar{y}) \propto \lambda^{n \bar{y}} \exp(-n\lambda)
$$
Which we recognise as an unnormalized $\Gamma(n\bar{y} +1, n)$ distribution in the shape-rate parametrization. We can the simulate from the posterior to compute a credible interval and posterior mean
```{r}
ybar <- before$y %>% mean()
n <- nrow(before)
m <- 10000
lambda_sim <- rgamma(m, n*ybar+1, n)
mean(lambda_sim)
quantile(lambda_sim, c(0.025, 0.975))
```

## 4.

Let $\bar{Y}_j$ be the killed drivers after the law is passed and let these also be i.i.d. with the old ones. Consider $Z$ to be the total number of killed drivers after law is passed. Since sums of independent poisson random variables and there are 23 observations, $Z \sim pois(23\lambda)$. We simulate $Z$ from the prior predictive, by first using our simulated lambdas, and then simulating Z from the relevant poisson distribution. We find the 0.1, 1 and 5 pct quantile.
```{r}
quantile(rpois(m, 23*lambda_sim), c(0.001,0.01,0.05))
```

## 5.

Under the above model considerations it is absurd to observe only the actual observed 119 deaths. This tells us that there should be an effect of the law.

## 6.

There seems to be a general downward trend for each year even before the law is passed, which is of course problematic when we assume that the rate is constant throughout years, and it muddys our conclusion on the effect of the law. 

## 7.
We do MCMC

```{r}
brm1A <- brm(y ~ 1, family=poisson(link="log"), data=before,
warmup = 500, iter = 1500, chains = 4, refresh = 0)
summary <- brm1A %>% summary()
exp(summary$fixed)
brm1A$prior
```
The difference to Q.3 is obviously that we are using a different link function for the poisson family. Partly also that we are using a prior. we get almost the same if we exp-transform.

## 8.

We can also do uniform priors. 
```{r}
brm1B <- brm(y ~ 1, family=poisson(), data=before,
prior = c(set_prior("uniform(-1000,1000)",
class="Intercept", lb=-1000, ub=1000)),
warmup = 500, iter = 1500, chains = 4, refresh = 0)
brm1B$prior
brm1B %>% summary()

brm1C <- brm(y ~ 1, family=poisson(link="identity"), data=before,
prior = c(set_prior("uniform(-1000,1000)",
class="Intercept", lb=-1000, ub=1000)),
warmup = 500, iter = 1500, chains = 4, refresh = 0)
brm1C$prior
brm1C %>% summary()
```
Again the difference between the two models is the link function used. Obviously we want the identity link to if we want a uniform prior on the $\lambda$ and not on the log-link scale. We might also run into numerical troubles on the log-link scale since we are taking a uniform interval over $[-1000,1000]$ and $e^{1000}$ is very, very large.

# B

We now assume that before the law $Y_i \sim pois(\lambda_i)$ with $\log(\lambda_i) = \alpha + \beta x_i$ with $x_i$ the number of months since data collection began. $Y_i$ are independent given the parameters

## 9.

The ML estimates for $\alpha$ and $\beta$ are computed with \texttt{glm}

```{r}
glm(y ~ x, family = poisson("log"), data = before) %>% coef()
```
## 10.

Assume the priors $ \alpha \sim \mathcal{N}(3,25)$ and $ \beta \sim \mathcal{N}(0,1)$. We simulate from the posterior with stan
```{r}
brm2 <- brm(y ~ x, family=poisson(link="log"), data=before,
prior = c(set_prior("normal(0,1)", class = "b", coef = "x"),
set_prior("normal(3,25)", class="Intercept")),
warmup = 500, iter = 1500, chains = 4,
refresh=1)

sim2 <- as.data.frame(brm2) %>% tibble()
```

The posterior means and intervals are calculated for $\beta$
```{r}
quantile(sim2$b_x, c(0.025, 0.975)) # -0.004497502 -0.002516153
mean(sim2$b_x) #-0.003501322
```
Hence with probability 0.95 $\beta \in [-0.004497502 ,-0.002516153]$, This does not cover 0, so there is evidence that there was a decrease before the law.

## 11.

Assume that the downward trend stays the same after the law has been passed, and that there is no effect of the law. Let $\bar{Y}_j$ be the killed drivers after the law is passed in each month and let $Z$ to be the total number of killed drivers after law is passed, i.e. $Z = \sum \bar{Y}_j$. Given $\alpha$ and $\beta$ $\bar{Y}_j \sim \pois(\lambda_j)$ with $\log(\lambda_j) = \alpha + \beta x_j$. Hence we can make draws from $Z$ by drawing for each simulated $\alpha$ and $\beta$ all $\bar{Y}_j$ and summing these to get $Z$.
```{r}
Z <- numeric(nrow(sim2))
for(i in 1:nrow(sim2)){
  alpha <- sim2[i,"b_Intercept"] %>% as.numeric()
  beta <- sim2[i,"b_x"] %>% as.numeric()
  lambda <- exp(after$x*beta + alpha)
  bar_Y <- rpois(length(lambda), lambda)
  Z[i] <- sum(bar_Y)
  }
hist(Z)
```
## 12.

Under the assumptions above we can compute how likely it is to observe 119 or fewer killings, 
```{r}
mean(Z <= 119)
```
This is still a low probability but not nearly as extreme as the one observed in question 5.

# C

## 1.

```{r}
plot(brm2)
```
This looks completely random/stable, no trends and no significant difference between chains. Very happy.

## 2.

```{r}
summary(brm2)
```
As stated in summary \hat{R}'s should be close to 1, they are indeed so we are happy.

## 3.

```{r}
acf(sim2$b_Intercept)
```
```{r}
acf(sim2$b_x)

```
Essentially 0 auto-correlation which is good. Large autocorrelations lead to less independent observations which can be a problem.

## 4.
```{r}
plot(brm2 , variable = "lp__")
```
```{r}
rhat(brm2)
```
```{r}
acf(sim2$lp__)
```
It behaves slightly less nicely but still looks fine.

## 5.

We find a posterior mean and confidence intervals for $\lambda_{169}$ using the same procedure as in part B
```{r}
lambdas <- exp(sim2$b_Intercept + sim2$b_x*169)
mean(lambdas) #7.050769
quantile(lambdas, c(0.025,0.975)) # 6.318982 7.817128 
```

## 6.

Let $\bar{x}$ be the month where the expected number of killed van drivers is 10. I.e. the month $x_i$ that solves the equation
$$
\exp(\alpha + \beta \bar{x}) = 10 \Leftrightarrow (\log(10)-\alpha)/\beta = \bar{x} = \bar{x}
$$
We can compute posterior confidence intervals and means easily as
```{r}
barx <- (log(10)-sim2$b_Intercept)/sim2$b_x
mean(barx)  %>% round(0) #69
quantile(barx, c(0.025,0.975))  %>% round(0) # 52 83
```
# D
We set up the stan code
```{r}
to_stan <- "data {
int N;
vector[N] x; 
int y[N];
}
parameters {
real alpha;
real beta;
}
transformed parameters {
vector[N] lambda = exp(alpha + beta*x);
real bar_x = 1/beta*(log(10)-alpha);
}
model {
alpha ~ normal(3,25);
beta ~ normal(0,1);
y ~ poisson(lambda);
}
"
write(to_stan, file = "van.stan")
```


#9.

Drawing from the distribution
```{r}
library(rstan)
data <- list(x=before$x, y =before$y, N = length(before$y))
fitted <- stan("van.stan", data = data)
```

## 10.

```{r}
results <- (fitted %>% summary())$summary %>% as.data.frame()
results["beta",]
```
We see a posterior mean of -0.00347 close to what we had before and a confidence interval of $[-0.004447479, -0.002472825]$ also close to what we had before.


## 11.

We find the following for $\lambda_{169}$

```{r}
results["lambda[169]",]
```
Also close to what we had before.

## 12.

We find for $\bar{x}$ that

```{r}
results["bar_x",]
```
Which is also what we had before.
